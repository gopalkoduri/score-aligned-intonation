{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import json, pickle\n",
    "from glob import glob\n",
    "from os.path import exists, expanduser\n",
    "from scipy.stats import variation, skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "\n",
    "home_dir = expanduser(\"~\")\n",
    "features_dir = home_dir + '/data/datasets/20-raagas/features/'\n",
    "dataset = json.load(file('{0}/data/datasets/20-raagas/dataset.json'.format(home_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svara positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append(home_dir+\"/workspace/PhD/intonation/evaluation\")\n",
    "import svara_positions as sp\n",
    "reload(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raaga_svara = json.load(file('{0}/data/raaga-svara.json'.format(home_dir)))\n",
    "svara_cents = json.load(file('{0}/data/svara-cents.json'.format(home_dir)))\n",
    "\n",
    "ground_truth = {}\n",
    "#raaga to cent-positions\n",
    "for raaga in raaga_svara:\n",
    "    svaras = set(raaga['arohana']).union(raaga['avarohana'])\n",
    "    ground_truth[raaga['rid']] = {'name': raaga['proper_name'],\n",
    "                                  'svara_positions': [svara_cents[svara] for svara in svaras]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histogram_truth = {}\n",
    "\n",
    "for rid, rdata in dataset.items():\n",
    "    histogram_truth[rid] = {}\n",
    "    for work_id, work_info in rdata['works'].items():\n",
    "        for rec in work_info['recordings']:\n",
    "            mbid = rec['mbid']\n",
    "            try:\n",
    "                params = pickle.load(file('{0}/histograms/{1}-params.pickle'.format(features_dir, mbid)))\n",
    "            except IOError:\n",
    "                print mbid\n",
    "            positions = [i for i in params.keys() if 0 <= i < 1200]\n",
    "            histogram_truth[rid][mbid] = sorted(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histogram_eval = sp.compare_to_groundtruth(histogram_truth, ground_truth)\n",
    "[p, r, f] = sp.eval_measures(histogram_eval)\n",
    "print p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "* Tonic errors are shifting all the peaks to lower/higher octaves\n",
    " - Non-octave error\n",
    "   * 0a33478e-d028-4f04-83a0-a9363adae895\n",
    "   * 64a4af23-ad15-4768-a9f1-252145b04dae\n",
    " - f2dc7763-0166-4fe9-830c-53b7953c2998 (very wierd)\n",
    " - e151896b-73ff-402f-a1bd-f64d5d99d3a6 (also wierd)\n",
    " - d42249e4-8889-4b34-a963-c99a674ddcf0\n",
    " - d2029d27-2e5f-42d1-9718-debf165f74fd\n",
    " - 43737160-f73a-4cc9-bfd6-be3a77600663\n",
    " - f70ca71a-af1f-400e-a597-1a409500a946\n",
    " - e27d0605-1426-45a4-b34f-a9520742cf53\n",
    " - and more...\n",
    "* Unidentified bumps, *a limitation*\n",
    " - d2387cfd-6996-4201-a4c0-06c37043e5a3 (at 315 cents)\n",
    " - caa8efc0-cc5d-4cf6-9a83-96ca53721ffe (315)\n",
    "* Error peaks due to liberal thresholds, *needs more fine tuning, redo the grid search from JNMR 2014, seems like 1e-04 for both work better though few valid peaks are missing (which can be compensated by choosing one peak per svara across octaves)*\n",
    " - A change in how we normalize has a significant impact on this, try various ways to normalize.\n",
    " - 70c49f3d-c339-4c4d-b29c-e38120711322 (both peak_amp_thresh and valley_thresh)\n",
    " - ecb3c485-ee24-4035-8a8a-3629b0a87673 (many peaks)\n",
    "* In Atana (rid: 6), there are peaks at Anya svaras (G2, N2) and often the peaks at svaras G3 and R3 are missing\n",
    " - This can be one possible conclusion that can be drawn from the fact that there are recurrent false positives\n",
    " - Asaveri (rid: 5 might be the same)\n",
    "* Raaga mislabels\n",
    "* Why doesn't the second bump get detected as peak, when it has better valley_thresh and peak_amp_thresh than the other which is recognized?: 89f21abd-ef27-4856-9553-dc0d194b0c7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = (16,8)\n",
    "\n",
    "ji_intervals = json.load(file(\"{0}/data/svara-cents.json\".format(home_dir)))\n",
    "ji_intervals = unique(sorted(ji_intervals.values()))\n",
    "ji_intervals = append(ji_intervals, ji_intervals[-12:]+1200)\n",
    "ji_intervals = append(ji_intervals, ji_intervals[-12:]+1200)\n",
    "ji_intervals = append(ji_intervals[:12]-1200, ji_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pickle.load(file(features_dir + 'histograms/cb280397-4e13-448e-9bd7-97105b2347dc.pickle'))\n",
    "data.plot(intervals=ji_intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_based_truth = {}\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "for rid, rdata in dataset.items():\n",
    "    rid = int(rid)\n",
    "    context_based_truth[rid] = {}\n",
    "    for work_id, work_info in rdata['works'].items():\n",
    "        for rec in work_info['recordings']:\n",
    "            mbid = rec['mbid']\n",
    "            pitchclass_counts = sp.get_counts(mbid)\n",
    "            if pitchclass_counts is None:\n",
    "                print mbid\n",
    "                continue\n",
    "            #Bound based on the significance of reletive counts\n",
    "            pos, neg = sp.segregate(pitchclass_counts, ground_truth[rid]['svara_positions'])\n",
    "            positives.extend(pos)\n",
    "            negatives.extend(neg)\n",
    "            \n",
    "            #The bound is somewhere around 0.25 normalized by max count.\n",
    "            positions = sp.get_legit_svaras(pitchclass_counts, norm_count_thresh=0.25)\n",
    "            context_based_truth[rid][mbid] = positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_based_eval = sp.compare_to_groundtruth(context_based_truth, ground_truth)\n",
    "[p, r, f] = sp.eval_measures(context_based_eval)\n",
    "print p, r, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rid = 2\n",
    "print ground_truth[rid]['name']\n",
    "\n",
    "tps = []\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "for rec in histogram_eval[rid]:\n",
    "    print rec[0], '\\t',\n",
    "    for s in rec[1]:\n",
    "        print len(s), '\\t',\n",
    "    tps.append(len(rec[1][0]))\n",
    "    fps.append(len(rec[1][1]))\n",
    "    fns.append(len(rec[1][2]))\n",
    "    print\n",
    "\n",
    "tps = sum(tps)\n",
    "fps = sum(fps)\n",
    "fns = sum(fns)\n",
    "p = tps/(tps+fps)\n",
    "r = tps/(tps+fns)\n",
    "print 'Precision: ', p\n",
    "print 'Recall: ', r\n",
    "print 'F1-score: ', 2*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raaga classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_svara_data(pickle_file, pitch_data):\n",
    "    svara_data = {}\n",
    "    try:\n",
    "        contours = pickle.load(file(pickle_file))\n",
    "        for k, v in contours.items():\n",
    "            pitch_contours = []\n",
    "            for i in v:\n",
    "                temp = pitch_data[i[0]:i[1], 1]\n",
    "                temp = temp[invert(isinf(temp))]\n",
    "                pitch_contours.append(temp)\n",
    "            svara_data[k] = [concatenate(pitch_contours)]\n",
    "            \n",
    "    except (IOError):\n",
    "        #print pickle_file + 'not found!'\n",
    "        return False\n",
    "    \n",
    "    return svara_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_parameters(svara_data):\n",
    "    parameters = {}\n",
    "    for svara, distribution in svara_data.items():\n",
    "        if svara < 0 or svara >= 1200:\n",
    "            continue\n",
    "            \n",
    "        [n, be] = np.histogram(distribution, bins=1200)\n",
    "        bc = (be[1:] + be[:-1])/2.0\n",
    "        \n",
    "        peak_pos = bc[argmax(n)]\n",
    "        peak_mean = float(mean(distribution))\n",
    "        peak_variance = float(variation(distribution, axis=1)[0])\n",
    "        peak_skew = float(skew(distribution, axis=1)[0])\n",
    "        peak_kurtosis = float(kurtosis(distribution, axis=1)[0])\n",
    "        pearson_skew = float(3.0 * (peak_mean - peak_pos) / np.sqrt(abs(peak_variance)))\n",
    "        parameters[svara] = {\"position\": float(peak_pos),\n",
    "                                \"mean\": peak_mean,\n",
    "                                \"amplitude\": float(max(n)),\n",
    "                                \"variance\": peak_variance,\n",
    "                                \"skew1\": peak_skew,\n",
    "                                \"skew2\": pearson_skew,\n",
    "                                \"kurtosis\": peak_kurtosis}\n",
    "        \n",
    "    all_amps = [parameters[svara][\"amplitude\"] for svara in parameters.keys()]\n",
    "    peak_amp_sum = sum(all_amps)\n",
    "    for svara in parameters.keys():\n",
    "        parameters[svara][\"amplitude\"] = parameters[svara][\"amplitude\"]/peak_amp_sum\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute parameters for Varnam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_dir = '/homedtic/gkoduri/data/intonation/varnam-analysis/recorded/audio/\n",
    "data_dir = '/home/gkoduri/Dropbox/UPF-Work/PhD/Varnam Analysis/data/audioScoreAlignment/'\n",
    "raagas = ['abhogi', 'begada', 'kalyani', 'mohanam', 'sahana', 'saveri', 'shree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chdir(data_dir)\n",
    "methods = ['semiautomatic', 'context_based']\n",
    "\n",
    "for raaga in raagas:\n",
    "    print raaga\n",
    "    chdir(raaga)\n",
    "    \n",
    "    artists = listdir('.')\n",
    "    artists = [a for a in artists if isdir(a)]\n",
    "    \n",
    "    # For each artist, for each svara, aggregate all the pitch values corresponding to the contour indices\n",
    "    for artist in artists:\n",
    "        print artist, \n",
    "        \n",
    "        try:\n",
    "            pitch_data = loadtxt(data_dir + raaga + '/' + artist + '/' + artist + '-cents.txt')\n",
    "        except(IOError):\n",
    "            continue\n",
    "        \n",
    "        for method in methods:\n",
    "            pickle_file = data_dir+raaga+'/'+artist+'/contours_'+method+'.pickle'\n",
    "            svara_data = get_svara_data(pickle_file, pitch_data)\n",
    "            if not svara_data: continue\n",
    "            \n",
    "            parameters = get_parameters(svara_data)\n",
    "            \n",
    "            if not exists(data_dir + '../parameters/' + method):\n",
    "                mkdir(data_dir + '../parameters/' + method)    \n",
    "            pickle.dump(parameters, file(data_dir + '../parameters/' + method + '/' + raaga + '_' + artist + '.pickle', 'w'))\n",
    "    print\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chdir(data_dir)\n",
    "method = 'phrase_aligned'\n",
    "\n",
    "for raaga in raagas:\n",
    "    print raaga\n",
    "    chdir(raaga)\n",
    "    \n",
    "    artists = listdir('.')\n",
    "    artists = [a for a in artists if isdir(a)]\n",
    "    \n",
    "    # For each artist, for each svara, aggregate all the pitch values corresponding to the contour indices\n",
    "    for artist in artists:\n",
    "        print artist,\n",
    "        try:\n",
    "            chdir('{0}/contours_phrase_aligned'.format(artist))\n",
    "        except(OSError):\n",
    "            print ' .. does not have contours!'\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            pitch_data = loadtxt(data_dir + raaga + '/' + artist + '/' + artist + '-cents.txt')\n",
    "        except(IOError):\n",
    "            print '.. does not have pitch file!'\n",
    "            continue\n",
    "            \n",
    "        pickled_files = glob('*')\n",
    "        \n",
    "        for f in pickled_files:\n",
    "            pickle_file = data_dir+raaga+'/'+artist+'/contours_phrase_aligned/'+f\n",
    "            svara_data = get_svara_data(pickle_file, pitch_data)\n",
    "            if not svara_data: \n",
    "                print ' .. wrong!'\n",
    "                continue\n",
    "            \n",
    "            parameters = get_parameters(svara_data)\n",
    "            \n",
    "            if not exists(data_dir + '../parameters/' + method):\n",
    "                mkdir(data_dir + '../parameters/' + method)\n",
    "            if not exists(data_dir + '../parameters/' + method + '/' + f[:-7]):\n",
    "                mkdir(data_dir + '../parameters/' + method + '/' + f[:-7])\n",
    "            pickle.dump(parameters, file(data_dir + '../parameters/' + method + '/' + f[:-7] + '/' + raaga + '_' + artist + '.pickle', 'w'))\n",
    "        \n",
    "        chdir('../..')\n",
    "        \n",
    "    print\n",
    "    chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def toWeka(pathGiven):\n",
    "    print pathGiven\n",
    "    parts = pathGiven.rstrip(\"/\").split(\"/\")\n",
    "    parentDir = \"/\".join(parts[:-1])\n",
    "    \n",
    "    arffFilename = \"/\".join([parentDir, parts[-1] + \".arff\"])\n",
    "    wekafile = file(arffFilename, \"w+\")\n",
    "\n",
    "    all_labels = ['abhogi', 'begada', 'kalyani', 'mohanam', 'sahana', 'saveri', 'shree']\n",
    "    svaras = pickle.load(file('/home/gkoduri/Dropbox/UPF-Work/PhD/Varnam Analysis/data/cents_to_svara_labels.pickle'))\n",
    "    svaras = sort(svaras.keys())\n",
    "    params = ['position', 'mean', 'amplitude', 'variance', 'skew1', 'skew2', 'kurtosis']\n",
    "    \n",
    "    descriptorList = []\n",
    "    for s in svaras:\n",
    "        for p in params:\n",
    "            descriptorList.append(p + '_' + str(s))\n",
    "    attr_list = \"\"\n",
    "    for i in descriptorList:\n",
    "        attr_list = attr_list + \"@ATTRIBUTE \" + i + \" REAL\\n\"\n",
    "    attr_list = attr_list + \"\\n@ATTRIBUTE segment {\" + \", \".join(all_labels) + \"}\\n\\n@DATA\\n\"\n",
    "    \n",
    "    wekafile.write(\"@RELATION raagaID\\n\\n\")\n",
    "    wekafile.write(attr_list)\n",
    "    \n",
    "    extension = \".pickle\"\n",
    "    fnames = listdir(pathGiven)\n",
    "    \n",
    "    for fname in fnames:\n",
    "        if fname.endswith(extension):\n",
    "            class_label = fname[:-7].split('_')[0]\n",
    "            fname = pathGiven + \"/\" + fname\n",
    "            parameters = pickle.load(file(fname))\n",
    "\n",
    "            #write the data points\n",
    "            data_entry = \"\"\n",
    "            for s in svaras:\n",
    "                if s in parameters.keys():\n",
    "                    for p in params:\n",
    "                        data_entry = data_entry + str(parameters[s][p]) + ', '\n",
    "                else:\n",
    "                    for p in params:\n",
    "                        data_entry = data_entry + '-10000, '\n",
    "            data_entry = data_entry + class_label\n",
    "            wekafile.write(data_entry + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toWeka(data_dir + '../parameters/context_based')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toWeka(data_dir + '../parameters/semiautomatic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "configs = glob('{0}/../parameters/phrase_aligned/*'.format(data_dir))\n",
    "\n",
    "for config in configs:\n",
    "    toWeka(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute parameters for Kriti dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_dir = '/homedtic/ssenturk/experiments/20-raagas/features/'\n",
    "contour_dir = '/homedtic/ssenturk/experiments/20-raagas/features/phrase_aligned/'\n",
    "align_dir = '/homedtic/ssenturk/experiments/20-raagas/features/noteAlignments/dtw_100centBinarization_kmeans/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contour_files = glob('{0}/*.pickle'.format(contour_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in contour_files:\n",
    "    if exists('{0}/{1}'.format(contour_dir, f.replace('contours', 'params'))):\n",
    "        continue\n",
    "        \n",
    "    mbid = basename(f)[:-16]\n",
    "    try:\n",
    "        pitch_data = loadtxt('{0}/pitch/{1}.txt'.format(feat_dir, mbid))\n",
    "    except(IOError):\n",
    "        print mbid, 'does not have pitch file!'\n",
    "        continue\n",
    "    svara_data = get_svara_data(f, pitch_data)\n",
    "    if not svara_data: \n",
    "        print f, 'cannot be read!'\n",
    "        continue\n",
    "\n",
    "    parameters = get_parameters(svara_data)\n",
    "    pickle.dump(parameters, file(f.replace('contours', 'params'), 'w'))\n",
    "    print f, 'is successful'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import basename\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def toWeka(pathGiven, mbids, mbid_raagas):\n",
    "    print pathGiven\n",
    "    parts = pathGiven.rstrip(\"/\").split(\"/\")\n",
    "    parentDir = \"/\".join(parts[:-1])\n",
    "    \n",
    "    arffFilename = \"/\".join([parentDir, parts[-1] + \".arff\"])\n",
    "    wekafile = file(arffFilename, \"w+\")\n",
    "\n",
    "    all_labels = [mbid_raagas[mbid] for mbid in mbids]\n",
    "    all_labels = sorted(unique(all_labels))\n",
    "    \n",
    "    svaras = json.load(file('/homedtic/gkoduri/data/svara-cents.json'))\n",
    "    svaras = array(svaras.values())\n",
    "    svaras = svaras[svaras >= 0]\n",
    "    svaras = svaras[svaras < 1200]\n",
    "    svaras = sort(unique(svaras))\n",
    "    params = ['position', 'mean', 'amplitude', 'variance', 'skew1', 'skew2', 'kurtosis']\n",
    "    \n",
    "    descriptorList = []\n",
    "    for s in svaras:\n",
    "        for p in params:\n",
    "            descriptorList.append(p + '_' + str(s))\n",
    "    attr_list = \"\"\n",
    "    for i in descriptorList:\n",
    "        attr_list = attr_list + \"@ATTRIBUTE \" + i + \" REAL\\n\"\n",
    "    attr_list = attr_list + \"\\n@ATTRIBUTE segment {\" + \", \".join(all_labels) + \"}\\n\\n@DATA\\n\"\n",
    "    \n",
    "    wekafile.write(\"@RELATION raagaID\\n\\n\")\n",
    "    wekafile.write(attr_list)\n",
    "    \n",
    "    extension = \"-params.pickle\"\n",
    "    for mbid in mbids:\n",
    "        fname = '{0}/{1}{2}'.format(pathGiven, mbid, extension)\n",
    "        class_label = mbid_raagas[mbid]\n",
    "        parameters = pickle.load(file(fname))\n",
    "\n",
    "        #write the data points\n",
    "        data_entry = \"\"\n",
    "        for s in svaras:\n",
    "            if s in parameters.keys():\n",
    "                for p in params:\n",
    "                    data_entry = data_entry + str(parameters[s][p]) + ', '\n",
    "            else:\n",
    "                for p in params:\n",
    "                    data_entry = data_entry + '-10000, '\n",
    "        data_entry = data_entry + class_label\n",
    "        wekafile.write(data_entry + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = json.load(file('/homedtic/ssenturk/experiments/20-raagas/dataset.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All MBIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mbid_raagas = {}\n",
    "for rid, rdata in dataset.items():\n",
    "    for wid, wdata in rdata['works'].items():\n",
    "        for rec in wdata['recordings']:\n",
    "            mbid_raagas[rec['mbid']] = unidecode(rdata['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/homedtic/ssenturk/experiments/20-raagas/features/phrase_aligned/'\n",
    "mbids = glob('{0}/*-params.pickle'.format(path))\n",
    "mbids = [basename(i) for i in mbids]\n",
    "mbids = [i[:-len('-params.pickle')] for i in mbids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toWeka(path, mbids, mbid_raagas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/homedtic/ssenturk/experiments/20-raagas/features/context_based/'\n",
    "toWeka(path, mbids, mbid_raagas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected MBIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(mbids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raaga_mbids = {}\n",
    "for mbid in mbids:\n",
    "    raaga = mbid_raagas[mbid]\n",
    "    if raaga in raaga_mbids.keys():\n",
    "        raaga_mbids[raaga].append(mbid)\n",
    "    else:\n",
    "        raaga_mbids[raaga] = [mbid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selected_mbids = []\n",
    "n_thresh = 5\n",
    "for r, rmbids in raaga_mbids.items():\n",
    "    if len(rmbids) >= n_thresh:\n",
    "        selected_mbids.extend(rmbids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savetxt('/homedtic/gkoduri/ismir-mbids.txt', selected_mbids, fmt='%s', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/homedtic/ssenturk/experiments/20-raagas/features/phrase_aligned/'\n",
    "toWeka(path, selected_mbids, mbid_raagas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/homedtic/ssenturk/experiments/20-raagas/features/context_based/'\n",
    "toWeka(path, selected_mbids, mbid_raagas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
